{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate transfer learning on Turnedtable Watertank (Dataset 2)  using an SVM Classifier: Self-supervised approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import h5py\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../../../')\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Tensorflow for GPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tensorflow Version: 2.2.0\n",
      "[INFO] Tensorflow built with CUDA\n",
      "[INFO] Number GPUs Available:  1\n",
      "[INFO] List of GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "print(\"[INFO] Tensorflow Version:\", tf.__version__)\n",
    "\n",
    "if tf.config.list_physical_devices(\"GPU\") and tf.test.is_built_with_cuda():\n",
    "    print(\"[INFO] Tensorflow built with CUDA\")\n",
    "    print(\"[INFO] Number GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "    print(\"[INFO] List of GPU devices:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "    physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "    # tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "else:\n",
    "    print(\"[ERROR] GPU not detected, make sure tensorflow-gpu is installed and that GPU is recognized\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "def flatten(x):\n",
    "    return x.reshape((x.shape[0], -1))\n",
    "\n",
    "def classSampling(X, y, samplesPerClass, numberOfClasses):\n",
    "    X_ret = np.zeros((samplesPerClass * numberOfClasses, X.shape[1]), dtype = np.float32)\n",
    "    y_ret = np.zeros((samplesPerClass * numberOfClasses), dtype = np.uint8)\n",
    "    count = 0\n",
    "\n",
    "    for classIdx in range(numberOfClasses):\n",
    "        indices = np.where(y == classIdx)[0]\n",
    "\n",
    "        #if len(indices) < samplesPerClass:\n",
    "        #    raise IndexError(\"Not enough samples for class {} to produce {} samples per class. Only {} class samples available\".format(classIdx, samplesPerClass, len(indices)))\n",
    "\n",
    "        doResample = len(indices) < samplesPerClass\n",
    "\n",
    "        chosenIndices = np.random.choice(indices, samplesPerClass, replace = doResample)\n",
    "\n",
    "        for ci in chosenIndices:\n",
    "            X_ret[count] = X[ci]\n",
    "            y_ret[count] = y[ci]\n",
    "\n",
    "            count += 1\n",
    "\n",
    "    return X_ret, y_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SonarTurnedTableSupervised(object):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def _normalize_images(self, images):\n",
    "        \"\"\"\n",
    "        Normalize sonar images by 1/255.\n",
    "        \"\"\"\n",
    "        return [element/255.0 for element in images]\n",
    "\n",
    "    def get_sonar_data(self):\n",
    "        \"\"\"\n",
    "        Reads from HDF5 file containing sonar data (resized to fix dims).\n",
    "        Returns list of np arrays containing image data.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"[INFO] Retrieving Sonar Turned Table Supervised Data\")\n",
    "\n",
    "        with h5py.File(self.file_path, \"r\") as f:\n",
    "            # list all groups\n",
    "            print(\"hdf5 dataset keys: %s\" % f.keys())\n",
    "\n",
    "            # get images and labels\n",
    "            x_train = f[\"x_train\"][...].astype(np.float32)\n",
    "            y_train = f[\"y_train\"][...]\n",
    "\n",
    "            x_test = f[\"x_test\"][...].astype(np.float32)\n",
    "            y_test = f[\"y_test\"][...]\n",
    "\n",
    "            _, x_val, _, y_val = train_test_split(x_test, y_test, train_size=0.5)\n",
    "\n",
    "            print(\"[INFO] Data dimensions\")\n",
    "            print(\"Train\", len(x_train))\n",
    "            print(\"Val\", len(x_val))\n",
    "            print(\"Test\", len(x_test))\n",
    "\n",
    "            # matias normalization\n",
    "            # multiply by 255 because hdf5 file comes as 1/255\n",
    "            x_train *= 255.0\n",
    "            x_val *= 255.0\n",
    "            x_test *= 255.0\n",
    "\n",
    "            x_train -= 84.51\n",
    "            x_val -= 84.51\n",
    "            x_test  -= 84.51\n",
    "\n",
    "        return (x_train, y_train), (x_val, y_val), (x_test, y_test)\n",
    "    \n",
    "def load_sonar_turnedtable_supervised(file_path):\n",
    "    \"\"\"\n",
    "    Loads test data from turnedtable dataset.\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"[INFO] Loading Tenorflow dataset\")\n",
    "\n",
    "    dataset_object = SonarTurnedTableSupervised(file_path)\n",
    "\n",
    "    # Read data\n",
    "    (x_train, y_train), (x_val, y_val), (x_test, y_test) = dataset_object.get_sonar_data()\n",
    "\n",
    "    # Train data\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=len(x_train)).batch(len(x_train))\n",
    "    train_dataset = train_dataset.prefetch(25)\n",
    "\n",
    "    # Validation data\n",
    "    # val_dataset = tf.data.Dataset.from_tensor_slices((x_val, labels_val))\n",
    "    # val_dataset = val_dataset.shuffle(buffer_size=len(x_val)).batch(batch_size)\n",
    "    # val_dataset = val_dataset.prefetch(25)\n",
    "\n",
    "    # Test data\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    test_dataset = test_dataset.shuffle(buffer_size=len(x_test)).batch(len(x_test)) # feed full test set\n",
    "    test_dataset = test_dataset.prefetch(25)\n",
    "\n",
    "    print()\n",
    "    print(\"[INFO] Tensorflow data dimensions\")\n",
    "    # print(train_dataset)\n",
    "    # print(val_dataset)\n",
    "    print(test_dataset)\n",
    "\n",
    "    # return train_dataset, val_dataset, test_dataset\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 96, 96, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 48, 48, 8)    400         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 48, 48, 8)    32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 24, 24, 8)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 24, 24, 32)   288         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 24, 24, 64)   2112        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 24, 24, 64)   18496       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 24, 24, 128)  0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 24, 24, 128)  512         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 24, 24, 32)   4128        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 24, 24, 64)   2112        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 24, 24, 64)   18496       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 24, 24, 128)  0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 24, 24, 128)  512         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 24, 24, 64)   8256        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 24, 24, 128)  8320        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 24, 24, 128)  73856       conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 24, 24, 256)  0           conv2d_8[0][0]                   \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 24, 24, 256)  1024        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 12, 12, 256)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 12, 12, 64)   16448       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 12, 12, 128)  8320        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 12, 12, 128)  73856       conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 12, 12, 256)  0           conv2d_11[0][0]                  \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 12, 12, 256)  1024        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 12, 12, 96)   24672       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 12, 12, 192)  18624       conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 12, 12, 192)  166080      conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 12, 12, 384)  0           conv2d_14[0][0]                  \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 12, 12, 384)  1536        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 12, 12, 96)   36960       batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 12, 12, 192)  18624       conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 12, 12, 192)  166080      conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 12, 12, 384)  0           conv2d_17[0][0]                  \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 12, 12, 384)  1536        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 12, 12, 128)  49280       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 12, 12, 256)  33024       conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 12, 12, 256)  295168      conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 12, 12, 512)  0           conv2d_20[0][0]                  \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 12, 12, 512)  2048        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 6, 6, 512)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 6, 6, 128)    65664       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 6, 6, 256)    33024       conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 6, 6, 256)    295168      conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 6, 6, 512)    0           conv2d_23[0][0]                  \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 6, 6, 512)    2048        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 6, 6, 4)      51204       batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 6, 6, 4)      16          conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 4)            0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 4)            0           global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 1,498,948\n",
      "Trainable params: 1,493,804\n",
      "Non-trainable params: 5,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from architectures.squeezenet import squeezenet\n",
    "\n",
    "PRETRAINED_NUM_CLASSES = 4 # 11 supervised, 4 self-supervised\n",
    "input_shape = [96, 96, 1]\n",
    "\n",
    "model_name = \"squeezenet\"\n",
    "pretraining_mode = \"self_supervised_learning\"\n",
    "# layers = [\"batch_norm_48\", \"batch_norm_49\", \"batch_norm_50\"] # TODO: check layer names\n",
    "layers = [\"batch_normalization_8\", \"batch_normalization_9\", \"global_average_pooling2d\"]\n",
    "model = squeezenet(input_shape, PRETRAINED_NUM_CLASSES)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-0.00544592 -0.01632494  0.01687813  0.04675952  0.01727858\n",
      "     0.08048141  0.04251303 -0.1139203 ]]\n",
      "\n",
      "  [[-0.09565575  0.10050267 -0.09226272 -0.11019079  0.02015136\n",
      "    -0.09789215 -0.02381014 -0.06458307]]\n",
      "\n",
      "  [[ 0.09331498 -0.01439317  0.01874527 -0.0148453  -0.07260352\n",
      "    -0.05460784  0.00416581  0.0907032 ]]\n",
      "\n",
      "  [[ 0.09892149 -0.04754139 -0.0133201   0.08658461  0.02463846\n",
      "    -0.06913388  0.09681627  0.02571903]]\n",
      "\n",
      "  [[ 0.03571258 -0.05388857 -0.05642048  0.00077745 -0.00709493\n",
      "     0.10368766 -0.10818358  0.02545564]]\n",
      "\n",
      "  [[-0.0533876  -0.06222044  0.01957692 -0.02066834  0.11260329\n",
      "    -0.02103198 -0.06476174  0.11315957]]\n",
      "\n",
      "  [[-0.03850563 -0.09236903 -0.05340952  0.04758163 -0.01292292\n",
      "     0.02692783 -0.04538411  0.09803072]]]\n",
      "\n",
      "\n",
      " [[[-0.05424626 -0.04718952  0.07769749  0.01299366 -0.08162719\n",
      "    -0.11072285 -0.08204272  0.07492848]]\n",
      "\n",
      "  [[ 0.02731937  0.03558099  0.02277398  0.04992607 -0.08838123\n",
      "     0.08094735 -0.11646322 -0.07953104]]\n",
      "\n",
      "  [[-0.03841574 -0.03013471 -0.1091724  -0.08518434 -0.05325709\n",
      "     0.11018179 -0.11151412 -0.0500392 ]]\n",
      "\n",
      "  [[ 0.00245618 -0.06290022 -0.03061438  0.04829112  0.08725685\n",
      "    -0.00732005  0.09048112  0.02890919]]\n",
      "\n",
      "  [[-0.05639567 -0.0258521   0.00320818  0.00270761  0.00388182\n",
      "    -0.11219724 -0.10217273 -0.00546984]]\n",
      "\n",
      "  [[ 0.01757829  0.04529393 -0.04233839 -0.08601759  0.07329677\n",
      "    -0.02840545  0.03636183  0.04533213]]\n",
      "\n",
      "  [[-0.03347699  0.08601482  0.05221556  0.07851475 -0.02349309\n",
      "     0.08684818  0.11599508 -0.03776936]]]\n",
      "\n",
      "\n",
      " [[[ 0.03390418 -0.08633204  0.11284426 -0.11065152  0.04375805\n",
      "     0.11346345  0.09055372  0.05234385]]\n",
      "\n",
      "  [[ 0.00355711  0.04662751 -0.11460086 -0.01361224  0.10063243\n",
      "     0.06509513  0.05451415  0.04237719]]\n",
      "\n",
      "  [[-0.11621939 -0.00961276  0.02471401  0.1101618   0.0844948\n",
      "     0.08721739  0.1081823  -0.04917727]]\n",
      "\n",
      "  [[-0.05657462 -0.05781547 -0.08485185 -0.08385006  0.03747135\n",
      "    -0.01800512  0.06627974  0.04540671]]\n",
      "\n",
      "  [[ 0.00209621 -0.09226499  0.10217342 -0.09582336  0.11040932\n",
      "    -0.04537482 -0.05185351 -0.03180741]]\n",
      "\n",
      "  [[ 0.03188801  0.02516064 -0.05685192 -0.10593958  0.07542995\n",
      "     0.03899102  0.03717849 -0.05500429]]\n",
      "\n",
      "  [[-0.01266326  0.06751733  0.0069259  -0.06693702  0.0548677\n",
      "     0.00240568 -0.09667356 -0.11313671]]]\n",
      "\n",
      "\n",
      " [[[-0.00451763  0.0752655   0.06308563  0.00875427  0.0876597\n",
      "     0.06551975 -0.09277533 -0.09849545]]\n",
      "\n",
      "  [[ 0.00623561  0.1069857  -0.05980517 -0.08141217  0.09859581\n",
      "     0.0349699  -0.07847744  0.04877573]]\n",
      "\n",
      "  [[ 0.03713498  0.05137907  0.06560574 -0.11529313  0.09868319\n",
      "     0.03340785 -0.01551061  0.10352609]]\n",
      "\n",
      "  [[ 0.10900174 -0.11360981  0.06910351  0.04383038 -0.04463261\n",
      "     0.01366466 -0.09422672 -0.09817289]]\n",
      "\n",
      "  [[-0.08665653  0.05201584 -0.05829674  0.05594039 -0.00112805\n",
      "     0.06638789 -0.10884242  0.04969731]]\n",
      "\n",
      "  [[-0.10464476 -0.04718935 -0.0462364  -0.09794963  0.07782041\n",
      "    -0.03715199  0.01235145 -0.0363954 ]]\n",
      "\n",
      "  [[-0.04576274 -0.04953571 -0.04602142  0.07401429 -0.03128845\n",
      "     0.07301755  0.09331678 -0.11162219]]]\n",
      "\n",
      "\n",
      " [[[ 0.04212211  0.04073265 -0.07621367 -0.02131956 -0.10303522\n",
      "     0.05831963  0.01130815 -0.01259833]]\n",
      "\n",
      "  [[ 0.00394639 -0.08462861 -0.09621781 -0.06437778  0.0952035\n",
      "    -0.09233457  0.09733053  0.08915934]]\n",
      "\n",
      "  [[ 0.10042645 -0.03108333 -0.06273642  0.00671988 -0.00916644\n",
      "     0.08692634  0.04527627  0.03708406]]\n",
      "\n",
      "  [[-0.04665221  0.06186892 -0.01626428 -0.08576334 -0.03129379\n",
      "     0.10323206 -0.09365492 -0.02553407]]\n",
      "\n",
      "  [[ 0.01597004 -0.07236144  0.06066392  0.01030266  0.09794429\n",
      "     0.07167938  0.01064806  0.06092805]]\n",
      "\n",
      "  [[-0.05622784 -0.0969158   0.04942884  0.01362659 -0.01070735\n",
      "    -0.06878328  0.07764798 -0.08184845]]\n",
      "\n",
      "  [[ 0.01829448 -0.00501039  0.06687587 -0.11424871  0.06909738\n",
      "     0.01843631 -0.07530849  0.08719912]]]\n",
      "\n",
      "\n",
      " [[[ 0.02976656 -0.04406738 -0.08792081 -0.05595538  0.06139264\n",
      "    -0.00181367  0.02102365 -0.01264928]]\n",
      "\n",
      "  [[-0.02278563 -0.03076635  0.08444193 -0.09976216  0.10014184\n",
      "     0.06029031  0.05952376  0.11542155]]\n",
      "\n",
      "  [[ 0.09979199  0.0152197   0.10102235  0.08994725  0.04698972\n",
      "    -0.07835708  0.10603689 -0.05098181]]\n",
      "\n",
      "  [[ 0.11421253  0.01359035 -0.08518378  0.00668687 -0.11359162\n",
      "     0.07364361  0.02125865  0.05578753]]\n",
      "\n",
      "  [[ 0.06565541  0.04585089 -0.11206122 -0.04737467  0.10521904\n",
      "    -0.096559   -0.03548291  0.10373485]]\n",
      "\n",
      "  [[-0.03519703 -0.07945648  0.0686944  -0.00351935  0.01705559\n",
      "     0.01249838 -0.04174641  0.10857886]]\n",
      "\n",
      "  [[ 0.11416261 -0.01809338  0.03452556 -0.0426832  -0.04327141\n",
      "    -0.03940041  0.097371    0.08822463]]]\n",
      "\n",
      "\n",
      " [[[ 0.05044219  0.03693829  0.03645575 -0.04123969 -0.09775221\n",
      "     0.08746815 -0.06794398 -0.07675424]]\n",
      "\n",
      "  [[ 0.0705696  -0.07550064 -0.00626606  0.04315796 -0.033269\n",
      "    -0.04348415 -0.06831636 -0.00463888]]\n",
      "\n",
      "  [[ 0.09507744 -0.09572427  0.00803909 -0.03712612 -0.03530693\n",
      "     0.03871487  0.0763659   0.00838906]]\n",
      "\n",
      "  [[-0.09493589 -0.02681804  0.00906511 -0.11434951  0.09706558\n",
      "    -0.10735081  0.04059775  0.10450307]]\n",
      "\n",
      "  [[ 0.07733351 -0.09200506 -0.10410245  0.03964961  0.11625084\n",
      "     0.08749501 -0.07575393  0.00041395]]\n",
      "\n",
      "  [[-0.09890267 -0.10702761 -0.09719902 -0.09646003 -0.07801296\n",
      "     0.06175427 -0.03194037  0.06876028]]\n",
      "\n",
      "  [[ 0.00376674  0.07968093  0.10380884 -0.02471154 -0.07052459\n",
      "    -0.01034143 -0.10146133  0.11326388]]]]\n"
     ]
    }
   ],
   "source": [
    "# check weights BEFORE loading pretrained model (second conv layer)\n",
    "print(model.layers[1].get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Pretrained checkpoint restored correctly: ../../../pretraining/results/self_supervised_learning/checkpoints/sonar1/squeezenet/batch_size_128/96x96_substract_mean_online_aug_width_32/ckpt-21\n"
     ]
    }
   ],
   "source": [
    "# path to pretrained model checkpoint\n",
    "pretrained_checkpoint_prefix = os.path.join(\"../../../pretraining/results/\" + pretraining_mode + \"/checkpoints/sonar1/\" + model_name + \"/batch_size_128/96x96_substract_mean_online_aug_width_32\")\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.5, epsilon=1)\n",
    "\n",
    "# define pretrained checkpoint model\n",
    "checkpoint_pretrained = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, model=model) # 4 ssl, 11 sl\n",
    "manager_pretrained = tf.train.CheckpointManager(checkpoint_pretrained, pretrained_checkpoint_prefix, max_to_keep=3)\n",
    "\n",
    "# restore model weights\n",
    "checkpoint_pretrained.restore(manager_pretrained.latest_checkpoint)\n",
    "\n",
    "if manager_pretrained.latest_checkpoint:\n",
    "    print(\"[INFO] Pretrained checkpoint restored correctly: {}\".format(manager_pretrained.latest_checkpoint))\n",
    "else:\n",
    "    print(\"[INFO] Could not restore pretrained checkpoint correctly, make sure path to pre-trained folder is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-0.06216595 -0.03900114 -0.02578199 -0.02512501 -0.00926176\n",
      "    -0.0772492   0.08437654  0.09840393]]\n",
      "\n",
      "  [[ 0.08783991 -0.10164145 -0.02722576 -0.08092385 -0.0152075\n",
      "    -0.09763503  0.01076004  0.02054841]]\n",
      "\n",
      "  [[-0.03476039 -0.01866436 -0.11986611  0.05831946 -0.11343325\n",
      "    -0.02424521 -0.06752407  0.00966931]]\n",
      "\n",
      "  [[-0.09956822 -0.04149697 -0.0852205   0.10161399 -0.02028412\n",
      "    -0.09849622 -0.01819678 -0.09657682]]\n",
      "\n",
      "  [[ 0.05221378  0.01381214  0.00276488  0.03069827  0.05144576\n",
      "    -0.00697406  0.05501994 -0.01501517]]\n",
      "\n",
      "  [[ 0.04223234  0.10458919 -0.04650391  0.00161301  0.02099225\n",
      "     0.10271543  0.11928242 -0.08690233]]\n",
      "\n",
      "  [[ 0.08221544 -0.0049354  -0.05628303  0.06656159  0.09399419\n",
      "     0.07261222 -0.02798264 -0.08347652]]]\n",
      "\n",
      "\n",
      " [[[ 0.00047691  0.07692079  0.01617629  0.07047198  0.08686607\n",
      "    -0.0433566   0.00916725  0.04945499]]\n",
      "\n",
      "  [[-0.02974954  0.01272425  0.0555886   0.09433492 -0.01387227\n",
      "     0.04432397  0.13563244  0.00651278]]\n",
      "\n",
      "  [[-0.09382562  0.05086119  0.03260311 -0.06769482 -0.09122989\n",
      "    -0.00130989  0.03932575 -0.01622847]]\n",
      "\n",
      "  [[ 0.07789835 -0.09162525  0.09772023  0.05353693 -0.0166718\n",
      "     0.0170392  -0.15307812 -0.02999679]]\n",
      "\n",
      "  [[ 0.08987559 -0.08390643  0.0488353   0.14044255  0.09991998\n",
      "    -0.01662789  0.06937123 -0.02093223]]\n",
      "\n",
      "  [[-0.0005806   0.03744551 -0.01508343  0.08454919  0.05786892\n",
      "    -0.10487786 -0.01957865  0.01616156]]\n",
      "\n",
      "  [[ 0.08663671 -0.02764871 -0.02431356  0.03264019  0.00920067\n",
      "     0.06694484  0.07593752 -0.02590648]]]\n",
      "\n",
      "\n",
      " [[[-0.00336525 -0.08971535 -0.10556893 -0.06839999  0.0140167\n",
      "     0.10113636 -0.00219026  0.1164541 ]]\n",
      "\n",
      "  [[-0.01466064  0.0647555  -0.07935384  0.0654368   0.01604722\n",
      "     0.05485763  0.11000086  0.11848119]]\n",
      "\n",
      "  [[-0.05346181  0.00886537 -0.02811771  0.02792971  0.08023463\n",
      "    -0.01804017 -0.01251898 -0.12987669]]\n",
      "\n",
      "  [[ 0.00918676  0.08656278 -0.1182083   0.05295856 -0.00965512\n",
      "     0.0240304  -0.13954116 -0.09486616]]\n",
      "\n",
      "  [[-0.01377564  0.03808655  0.0763118  -0.07706346 -0.05352079\n",
      "    -0.005884    0.06034947  0.02062404]]\n",
      "\n",
      "  [[ 0.01307618 -0.12962614  0.04754759  0.08086514 -0.05897726\n",
      "    -0.08971314  0.05431144 -0.06359162]]\n",
      "\n",
      "  [[-0.04123626 -0.08088776  0.05449654  0.11571553 -0.03982627\n",
      "     0.02824811 -0.03310224 -0.11374069]]]\n",
      "\n",
      "\n",
      " [[[-0.08862807 -0.0813889  -0.0853465   0.00373474 -0.08279965\n",
      "     0.1056815  -0.0318942   0.07721613]]\n",
      "\n",
      "  [[-0.08801147 -0.10723839 -0.12054691 -0.02722923 -0.09284468\n",
      "    -0.09159534  0.18115692 -0.0555298 ]]\n",
      "\n",
      "  [[-0.07685504  0.03694799 -0.10908262 -0.05746757  0.04206384\n",
      "    -0.07382233 -0.0724059  -0.07061235]]\n",
      "\n",
      "  [[ 0.07600513  0.00217225 -0.0749086  -0.07775234 -0.0281934\n",
      "     0.07098035 -0.11148663 -0.02164344]]\n",
      "\n",
      "  [[-0.05756436  0.08229234  0.0228521  -0.03734843 -0.08112086\n",
      "     0.0964132  -0.01314992 -0.06044219]]\n",
      "\n",
      "  [[ 0.01670164 -0.00191012 -0.10740699 -0.07857814  0.02310259\n",
      "     0.08709194  0.02608144 -0.07831217]]\n",
      "\n",
      "  [[ 0.16404758  0.1112401  -0.02839459 -0.0756636   0.00616734\n",
      "     0.00107148 -0.02774877  0.07036071]]]\n",
      "\n",
      "\n",
      " [[[ 0.07144514 -0.08117114 -0.07637449 -0.05001972  0.0954613\n",
      "     0.12362161  0.09254483  0.04905485]]\n",
      "\n",
      "  [[-0.11578733 -0.04948206  0.11225109 -0.00058107 -0.01294626\n",
      "     0.0440176   0.04005275 -0.06385448]]\n",
      "\n",
      "  [[-0.07193763 -0.03941956  0.08577305 -0.02636348 -0.07950499\n",
      "     0.08762645 -0.10394501 -0.06988747]]\n",
      "\n",
      "  [[ 0.06891607 -0.09341794  0.1322555  -0.0706004   0.02633542\n",
      "     0.03649662 -0.04073945 -0.0718092 ]]\n",
      "\n",
      "  [[-0.07357071  0.00975771  0.06811254 -0.06113302  0.10725952\n",
      "    -0.00797262 -0.09944481 -0.1178754 ]]\n",
      "\n",
      "  [[-0.05476837 -0.04261877  0.10902675 -0.09325965  0.09603374\n",
      "     0.08867337  0.02008155 -0.00998765]]\n",
      "\n",
      "  [[ 0.14943181 -0.08793277 -0.07577278 -0.08915956  0.11990649\n",
      "     0.06402555  0.06693425  0.07189989]]]\n",
      "\n",
      "\n",
      " [[[-0.11962008  0.11493124 -0.07038314 -0.04220145  0.09965012\n",
      "     0.13942829  0.04978712 -0.07884744]]\n",
      "\n",
      "  [[ 0.02368857  0.04521363 -0.01461681 -0.02206239  0.11576346\n",
      "    -0.07493913  0.00664408  0.01105502]]\n",
      "\n",
      "  [[ 0.09046371  0.03049804  0.11417156  0.02470006  0.0928742\n",
      "     0.07449861  0.012953   -0.01136455]]\n",
      "\n",
      "  [[-0.04092547 -0.03107867  0.13740948 -0.03996482  0.1029862\n",
      "     0.07174299 -0.04961502  0.03130847]]\n",
      "\n",
      "  [[-0.10717218 -0.06281631  0.10546879  0.05829464 -0.08847052\n",
      "     0.08099836  0.05058083 -0.0546883 ]]\n",
      "\n",
      "  [[ 0.00762324 -0.02629613  0.0723132   0.02778878  0.04680789\n",
      "     0.04900672 -0.02911246  0.07900856]]\n",
      "\n",
      "  [[ 0.10065763 -0.06226122 -0.02330411  0.13557148  0.07101759\n",
      "    -0.09515402 -0.09738076  0.07174305]]]\n",
      "\n",
      "\n",
      " [[[-0.01969322  0.15528162  0.05891782  0.03701206 -0.04085656\n",
      "     0.03012499  0.05514098 -0.07091851]]\n",
      "\n",
      "  [[ 0.05022334  0.14437577  0.10704787 -0.10088713 -0.01948345\n",
      "     0.09284694 -0.02303923  0.10167807]]\n",
      "\n",
      "  [[ 0.04418651  0.17424922 -0.07800157 -0.00646284 -0.06335219\n",
      "     0.04584314 -0.07513222 -0.01943853]]\n",
      "\n",
      "  [[ 0.01343621 -0.03256495  0.10781389  0.01093308 -0.07847645\n",
      "    -0.01996314 -0.02016174  0.01191623]]\n",
      "\n",
      "  [[-0.07032696  0.02547353  0.09255808  0.02148153 -0.07345828\n",
      "    -0.0363452  -0.074207   -0.05668457]]\n",
      "\n",
      "  [[ 0.08266357  0.05332749  0.06735127 -0.06113046  0.02805828\n",
      "    -0.10892268 -0.0930634  -0.02951322]]\n",
      "\n",
      "  [[ 0.11284932 -0.0493842   0.0233286  -0.08522145  0.04532268\n",
      "     0.11418355  0.06239653  0.09182546]]]]\n"
     ]
    }
   ],
   "source": [
    "# check weights AFTER loading pretrained model (second conv layer)\n",
    "print(model.layers[1].get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models up to intermediate layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate layers from model: ['batch_normalization_8', 'batch_normalization_9', 'global_average_pooling2d']\n"
     ]
    }
   ],
   "source": [
    "print(\"Intermediate layers from model:\", layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/63297838/how-can-i-obtain-the-output-of-an-intermediate-layer-feature-extraction\n",
    "batch_normalization_8 = tf.keras.Model(inputs=model.get_layer(\"input_1\").output, \n",
    "                                     outputs=model.get_layer(layers[0]).output)\n",
    "\n",
    "batch_normalization_9 = tf.keras.Model(inputs=model.get_layer(\"input_1\").output, \n",
    "                                     outputs=model.get_layer(layers[1]).output)\n",
    "\n",
    "global_average_pooling2d = tf.keras.Model(inputs=model.get_layer(\"input_1\").output, \n",
    "                                     outputs=model.get_layer(layers[2]).output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate vector embeddings for train and test data (up to n-th layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Loading Tenorflow dataset\n",
      "[INFO] Retrieving Sonar Turned Table Supervised Data\n",
      "hdf5 dataset keys: <KeysViewHDF5 ['class_names', 'x_test', 'x_train', 'y_test', 'y_train']>\n",
      "[INFO] Data dimensions\n",
      "Train 1505\n",
      "Val 323\n",
      "Test 645\n",
      "\n",
      "[INFO] Tensorflow data dimensions\n",
      "<PrefetchDataset shapes: ((None, 96, 96, 1), (None,)), types: (tf.float32, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "# define tensorflow dataset\n",
    "data_dir = \"../../../../../../datasets/sonar_turntable_dataset_2/marine-debris-turntable-classification-object_classes-platform-96x96.hdf5\"\n",
    "train_dataset, test_dataset = load_sonar_turnedtable_supervised(data_dir)\n",
    "\n",
    "# load tensorflow tensors individually\n",
    "x_train, y_train = next(iter(train_dataset))\n",
    "x_test, y_test = next(iter(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1505,24,24,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/rotnet/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m    927\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_FallbackException\u001b[0m: Expecting int64_t value for attr strides, got numpy.int32",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-523ba95d7437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_train_batch_normalization_8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_normalization_8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_train_batch_normalization_9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_normalization_9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx_train_global_average_pooling2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_average_pooling2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mx_test_batch_normalization_8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_normalization_8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rotnet/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rotnet/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    717\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    718\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rotnet/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rotnet/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rotnet/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    205\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rotnet/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m   1104\u001b[0m           call_from_convolution=False)\n\u001b[1;32m   1105\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rotnet/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rotnet/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         name=self.name)\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rotnet/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)\u001b[0m\n\u001b[1;32m   2012\u001b[0m                            \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m                            \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2014\u001b[0;31m                            name=name)\n\u001b[0m\u001b[1;32m   2015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rotnet/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m             data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m    934\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rotnet/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_eager_fallback\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[1;32m   1021\u001b[0m   _result = _execute.execute(b\"Conv2D\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0;32m-> 1022\u001b[0;31m                              ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m   1023\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m     _execute.record_gradient(\n",
      "\u001b[0;32m~/anaconda3/envs/rotnet/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1505,24,24,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]"
     ]
    }
   ],
   "source": [
    "# perform a forward pass to generate embeddings (both train and test data) (for each n-th layer)\n",
    "\n",
    "# NOTE: when doing this all tensors are loaded into memory, this saturates NVIDIA memory (nvidia-smi)\n",
    "x_train_batch_normalization_8 = batch_normalization_8([x_train], training=False)\n",
    "x_train_batch_normalization_9 = batch_normalization_9([x_train], training=False)\n",
    "x_train_global_average_pooling2d = global_average_pooling2d([x_train], training=False)\n",
    "\n",
    "x_test_batch_normalization_8 = batch_normalization_8([x_test], training=False)\n",
    "x_test_batch_normalization_9 = batch_normalization_9([x_test], training=False)\n",
    "x_test_global_average_pooling2d = global_average_pooling2d([x_test], training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1505, 6, 6, 512), dtype=float32, numpy=\n",
       "array([[[[1.21386254e+00, 4.89381775e-02, 8.45410645e-01, ...,\n",
       "          0.00000000e+00, 4.96995270e-01, 9.05933157e-02],\n",
       "         [1.40980411e+00, 1.15261090e+00, 8.35694194e-01, ...,\n",
       "          0.00000000e+00, 5.93474329e-01, 4.82767463e-01],\n",
       "         [1.82574883e-01, 6.56372130e-01, 5.60133994e-01, ...,\n",
       "          0.00000000e+00, 6.67990267e-01, 9.42384839e-01],\n",
       "         [1.29431516e-01, 1.25769377e+00, 2.52908558e-01, ...,\n",
       "          0.00000000e+00, 5.47911167e-01, 1.50075305e+00],\n",
       "         [0.00000000e+00, 7.05164850e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 3.08221430e-01, 7.03265250e-01],\n",
       "         [1.43316180e-01, 8.97599578e-01, 6.38682246e-01, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 6.05568409e-01]],\n",
       "\n",
       "        [[1.58779538e+00, 2.70380199e-01, 6.62188411e-01, ...,\n",
       "          0.00000000e+00, 9.18517351e-01, 0.00000000e+00],\n",
       "         [1.36984611e+00, 1.52387643e+00, 6.65007174e-01, ...,\n",
       "          5.64474225e-01, 1.26944888e+00, 0.00000000e+00],\n",
       "         [1.15218312e-01, 1.10056651e+00, 0.00000000e+00, ...,\n",
       "          9.88080502e-01, 8.42722595e-01, 0.00000000e+00],\n",
       "         [2.41711080e-01, 6.82391763e-01, 1.18062049e-01, ...,\n",
       "          8.92391920e-01, 9.17503238e-01, 3.38331610e-01],\n",
       "         [8.01368356e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.33009458e+00, 6.31139040e-01, 9.87581849e-01],\n",
       "         [3.50163549e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.07743216e+00, 4.88872319e-01, 0.00000000e+00]],\n",
       "\n",
       "        [[1.67930317e+00, 0.00000000e+00, 6.10883594e-01, ...,\n",
       "          0.00000000e+00, 6.40770674e-01, 0.00000000e+00],\n",
       "         [1.31692612e+00, 1.33257222e+00, 0.00000000e+00, ...,\n",
       "          2.02301934e-01, 7.47570693e-01, 0.00000000e+00],\n",
       "         [3.54028255e-01, 1.43480206e+00, 0.00000000e+00, ...,\n",
       "          1.42425289e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "         [4.69609290e-01, 7.73074746e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 3.37829918e-01, 0.00000000e+00],\n",
       "         [7.43504584e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          8.55491638e-01, 7.40871966e-01, 0.00000000e+00],\n",
       "         [4.51024294e-01, 0.00000000e+00, 5.22799492e-01, ...,\n",
       "          1.58414018e+00, 0.00000000e+00, 3.75580698e-01]],\n",
       "\n",
       "        [[7.55223513e-01, 2.43446156e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 1.44895494e+00, 0.00000000e+00],\n",
       "         [2.37992689e-01, 7.69590557e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 7.24948108e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 6.02175832e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 2.17057258e-01, 3.09611231e-01],\n",
       "         [2.29788959e-01, 5.17907739e-01, 0.00000000e+00, ...,\n",
       "          2.76066422e-01, 5.44403076e-01, 0.00000000e+00],\n",
       "         [3.90599757e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          6.11351252e-01, 3.87470573e-01, 0.00000000e+00],\n",
       "         [5.69515169e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          3.46275985e-01, 0.00000000e+00, 2.04475895e-01]],\n",
       "\n",
       "        [[6.03065133e-01, 1.81254387e-01, 1.62985653e-01, ...,\n",
       "          4.45793033e-01, 1.15529406e+00, 0.00000000e+00],\n",
       "         [7.56154954e-01, 3.09719771e-01, 0.00000000e+00, ...,\n",
       "          1.91156042e+00, 1.20842910e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 5.65345109e-01, 0.00000000e+00, ...,\n",
       "          1.81911242e+00, 0.00000000e+00, 3.70041668e-01],\n",
       "         [0.00000000e+00, 2.00629160e-01, 0.00000000e+00, ...,\n",
       "          1.40022433e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          7.48094797e-01, 0.00000000e+00, 3.92494977e-01],\n",
       "         [1.10064581e-01, 0.00000000e+00, 3.84586662e-01, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 7.39752233e-01]],\n",
       "\n",
       "        [[8.80364239e-01, 1.38908774e-01, 9.69308019e-02, ...,\n",
       "          1.00734603e+00, 1.31476969e-01, 1.56573400e-01],\n",
       "         [6.56927526e-01, 0.00000000e+00, 5.46266437e-01, ...,\n",
       "          1.70416558e+00, 8.26609254e-01, 3.44359756e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.23115039e+00, 8.10497880e-01, 1.03638220e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.85691655e+00, 4.63376671e-01, 4.76679295e-01],\n",
       "         [3.30108911e-01, 0.00000000e+00, 3.68238956e-01, ...,\n",
       "          1.32343161e+00, 7.18894958e-01, 0.00000000e+00],\n",
       "         [4.51950848e-01, 0.00000000e+00, 1.56692922e-01, ...,\n",
       "          8.26672494e-01, 0.00000000e+00, 0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[0.00000000e+00, 1.52691096e-01, 5.29016376e-01, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 1.43502966e-01, 4.82692480e-01, ...,\n",
       "          0.00000000e+00, 8.17698956e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 3.00359893e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 2.86733508e-01, 0.00000000e+00, ...,\n",
       "          1.28555104e-01, 2.33236456e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 3.96128833e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 1.41390359e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          3.07653040e-01, 3.29929680e-01, 1.17597485e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 1.70783386e-01, ...,\n",
       "          0.00000000e+00, 4.06205714e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 3.19671243e-01, 4.21145827e-01, ...,\n",
       "          0.00000000e+00, 1.83619052e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 1.11542404e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 6.32457376e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 2.08643031e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 2.02846766e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          8.94382715e-01, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[2.85702109e-01, 0.00000000e+00, 1.24083459e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 4.92233336e-01, 9.89120781e-01, ...,\n",
       "          0.00000000e+00, 2.70563632e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 8.75124216e-01, ...,\n",
       "          1.38954234e+00, 1.76084387e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 4.49925661e-01, ...,\n",
       "          9.02132750e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.55398309e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.70396245e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[7.95933366e-01, 0.00000000e+00, 6.79903626e-01, ...,\n",
       "          0.00000000e+00, 1.64485350e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 5.54022312e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.83697939e+00, 4.69868422e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 2.08654076e-01, ...,\n",
       "          5.78733504e-01, 3.30802470e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.32641780e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          9.08220828e-01, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[1.18128037e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [1.36451232e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 5.90511560e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.36809742e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          8.56128395e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.27201509e+00, 0.00000000e+00, 2.52235115e-01],\n",
       "         [0.00000000e+00, 1.98683381e-01, 0.00000000e+00, ...,\n",
       "          1.79942012e-01, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[1.23751953e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [5.72776973e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          6.75177157e-01, 2.88074195e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 1.84128478e-01, ...,\n",
       "          1.88957775e+00, 0.00000000e+00, 3.07772666e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 9.18155491e-01, ...,\n",
       "          1.23943233e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          9.46942747e-01, 0.00000000e+00, 1.76285785e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.33068621e+00, 0.00000000e+00, 6.05253614e-02]]],\n",
       "\n",
       "\n",
       "       [[[4.97658908e-01, 2.26412073e-01, 1.28878057e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 1.04931021e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 8.94786954e-01, 6.15833402e-01, ...,\n",
       "          2.00244635e-01, 1.10513580e+00, 5.58649361e-01],\n",
       "         [1.03040911e-01, 1.11017883e+00, 1.30273789e-01, ...,\n",
       "          0.00000000e+00, 1.09344733e+00, 5.84765911e-01],\n",
       "         [1.42691910e-01, 1.03211796e+00, 2.64871195e-02, ...,\n",
       "          0.00000000e+00, 3.89539629e-01, 4.24094677e-01],\n",
       "         [2.33292338e-02, 9.52406645e-01, 4.16681141e-01, ...,\n",
       "          0.00000000e+00, 5.58226347e-01, 8.26527923e-02]],\n",
       "\n",
       "        [[9.22215819e-01, 4.14413184e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 8.82159829e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 4.50354218e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 8.28124762e-01, 0.00000000e+00, ...,\n",
       "          1.10234261e+00, 1.60742247e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.12121868e+00, 7.57788479e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          3.03214610e-01, 1.14583671e+00, 0.00000000e+00],\n",
       "         [6.59933865e-01, 1.97398141e-01, 0.00000000e+00, ...,\n",
       "          8.70444536e-01, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[1.05376267e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 1.39320302e+00, 1.40793100e-02, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 2.36519694e-01, ...,\n",
       "          1.47311008e+00, 5.35422921e-01, 0.00000000e+00],\n",
       "         [4.51415032e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.23708463e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 2.65599966e-01, 0.00000000e+00],\n",
       "         [3.99715692e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[5.43277085e-01, 5.69180727e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 1.39797926e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          2.28812009e-01, 0.00000000e+00, 2.07650233e-02],\n",
       "         [0.00000000e+00, 2.22789854e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 5.73306441e-01, 0.00000000e+00, ...,\n",
       "          5.03833853e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 3.39736611e-01, ...,\n",
       "          3.98845226e-01, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[2.76941031e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 5.58346689e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 7.25047827e-01, ...,\n",
       "          0.00000000e+00, 4.95000660e-01, 2.49710783e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 8.41210335e-02, 0.00000000e+00, ...,\n",
       "          2.22059980e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 1.73117429e-01, 0.00000000e+00, ...,\n",
       "          1.69370547e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "         [3.40151966e-01, 1.69803277e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[3.01103026e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.59076020e-01, 0.00000000e+00, 7.77833819e-01],\n",
       "         [2.64523566e-01, 0.00000000e+00, 7.33150840e-01, ...,\n",
       "          9.28520977e-01, 0.00000000e+00, 1.98531970e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.54606104e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          9.46149707e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.36316240e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [3.97520751e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          9.34823036e-01, 0.00000000e+00, 2.77273625e-01]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[5.54868579e-01, 0.00000000e+00, 2.09653452e-01, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [9.95802283e-01, 2.33481854e-01, 4.39013660e-01, ...,\n",
       "          0.00000000e+00, 6.85738504e-01, 0.00000000e+00],\n",
       "         [6.18452132e-01, 2.78394729e-01, 8.34464669e-01, ...,\n",
       "          5.90230152e-02, 9.04665351e-01, 0.00000000e+00],\n",
       "         [8.51163507e-01, 8.32234383e-01, 8.65169168e-01, ...,\n",
       "          2.30508029e-01, 4.21075702e-01, 1.07643473e+00],\n",
       "         [3.37417990e-01, 8.76152217e-01, 8.20224956e-02, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 1.51392376e+00],\n",
       "         [0.00000000e+00, 1.17680086e-02, 5.58582425e-01, ...,\n",
       "          5.43553233e-01, 2.61656735e-02, 1.06153272e-01]],\n",
       "\n",
       "        [[7.40259111e-01, 6.23618126e-01, 1.59319758e-01, ...,\n",
       "          0.00000000e+00, 8.03648084e-02, 0.00000000e+00],\n",
       "         [1.08002603e+00, 1.34148788e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [7.14338124e-01, 6.62054271e-02, 0.00000000e+00, ...,\n",
       "          6.33005321e-01, 1.70548058e+00, 1.08340038e-02],\n",
       "         [3.38811427e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          6.49380326e-01, 1.82285869e+00, 8.91632855e-01],\n",
       "         [5.66983461e-01, 2.70252258e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 9.21631515e-01, 5.82605481e-01],\n",
       "         [0.00000000e+00, 1.27941728e-01, 1.28289675e-02, ...,\n",
       "          6.45930767e-01, 5.37137985e-01, 0.00000000e+00]],\n",
       "\n",
       "        [[1.65202153e+00, 1.92711979e-01, 3.46427023e-01, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [6.58399940e-01, 0.00000000e+00, 3.79514545e-01, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [1.82154924e-01, 4.51045722e-01, 0.00000000e+00, ...,\n",
       "          8.88791382e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.85798681e+00, 1.71447486e-01, 1.48875400e-01],\n",
       "         [0.00000000e+00, 2.08903059e-01, 0.00000000e+00, ...,\n",
       "          1.14160225e-01, 4.17020977e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 2.79270977e-01, ...,\n",
       "          7.10267484e-01, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[1.40898919e+00, 6.93294346e-01, 3.30813453e-02, ...,\n",
       "          0.00000000e+00, 4.08204645e-02, 0.00000000e+00],\n",
       "         [3.68216962e-01, 1.01068199e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 5.85913122e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 2.31343973e-02, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 5.75643480e-01, 0.00000000e+00, ...,\n",
       "          5.39589226e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 6.87130272e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 1.52528536e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 1.01818192e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 3.86673868e-01]],\n",
       "\n",
       "        [[1.16354299e+00, 2.07711473e-01, 1.53946295e-01, ...,\n",
       "          0.00000000e+00, 5.13336420e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 1.01600027e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 3.42235357e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 2.52194881e-01, 0.00000000e+00, ...,\n",
       "          1.03011453e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 4.84673083e-01, 0.00000000e+00, ...,\n",
       "          1.25499861e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 2.49452367e-01, 0.00000000e+00, ...,\n",
       "          5.65113485e-01, 0.00000000e+00, 2.97164977e-01],\n",
       "         [0.00000000e+00, 4.77514923e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 4.52172071e-01]],\n",
       "\n",
       "        [[7.69353747e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          6.34844124e-01, 0.00000000e+00, 1.95097715e-01],\n",
       "         [1.21529236e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          4.65871066e-01, 1.59877092e-01, 5.56281447e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          7.98413515e-01, 1.84804350e-01, 6.31211221e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.20745170e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 1.88785389e-01, ...,\n",
       "          7.97446847e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "         [1.52627841e-01, 4.34158117e-01, 4.62248385e-01, ...,\n",
       "          7.30552912e-01, 0.00000000e+00, 2.51083910e-01]]],\n",
       "\n",
       "\n",
       "       [[[6.53113246e-01, 4.54290479e-01, 3.25090051e-01, ...,\n",
       "          0.00000000e+00, 2.46574238e-01, 0.00000000e+00],\n",
       "         [7.92538345e-01, 0.00000000e+00, 6.71231508e-01, ...,\n",
       "          6.18359685e-01, 3.42639357e-01, 0.00000000e+00],\n",
       "         [3.01716439e-02, 1.43768096e+00, 3.01402528e-02, ...,\n",
       "          1.29568911e+00, 7.55373776e-01, 7.48455942e-01],\n",
       "         [7.97515333e-01, 9.62288976e-01, 7.14009881e-01, ...,\n",
       "          8.15240979e-01, 1.16498959e+00, 1.00717831e+00],\n",
       "         [5.90333879e-01, 6.40912652e-01, 4.67910796e-01, ...,\n",
       "          5.10940254e-01, 4.57203656e-01, 7.22755194e-01],\n",
       "         [1.75583541e-01, 1.72066256e-01, 1.05659497e+00, ...,\n",
       "          2.87493795e-01, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[8.43252718e-01, 0.00000000e+00, 4.57172245e-02, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 3.48493516e-01],\n",
       "         [6.10471547e-01, 1.45302888e-03, 0.00000000e+00, ...,\n",
       "          1.18889856e+00, 2.53336817e-01, 1.25075087e-01],\n",
       "         [2.51874954e-01, 6.40163124e-01, 0.00000000e+00, ...,\n",
       "          2.05259895e+00, 1.80451882e+00, 7.60612369e-01],\n",
       "         [9.21549618e-01, 7.70189703e-01, 9.86776352e-02, ...,\n",
       "          1.42471516e+00, 1.01065588e+00, 1.63200998e+00],\n",
       "         [6.41722500e-01, 7.38803446e-01, 0.00000000e+00, ...,\n",
       "          6.47588670e-01, 9.90402699e-01, 1.14861214e+00],\n",
       "         [7.94853568e-02, 4.89017427e-01, 0.00000000e+00, ...,\n",
       "          9.27468598e-01, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[1.07835996e+00, 0.00000000e+00, 4.65320289e-01, ...,\n",
       "          0.00000000e+00, 9.68485832e-01, 0.00000000e+00],\n",
       "         [1.16809785e+00, 0.00000000e+00, 4.48173463e-01, ...,\n",
       "          5.73757648e-01, 1.28421974e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 9.54543471e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 5.18168509e-02, 3.44119132e-01],\n",
       "         [0.00000000e+00, 9.16096866e-01, 3.64798844e-01, ...,\n",
       "          1.20460756e-01, 2.30051920e-01, 1.27440298e+00],\n",
       "         [0.00000000e+00, 1.08253360e+00, 4.85471904e-01, ...,\n",
       "          4.86900285e-02, 5.25438488e-01, 2.10460052e-01],\n",
       "         [0.00000000e+00, 1.08314276e+00, 0.00000000e+00, ...,\n",
       "          6.67749107e-01, 0.00000000e+00, 5.29617257e-03]],\n",
       "\n",
       "        [[0.00000000e+00, 5.30756533e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 1.34827936e+00, 0.00000000e+00],\n",
       "         [5.59777737e-01, 8.74446690e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 3.42847204e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 2.23623180e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 2.22674847e+00, 9.22094941e-01],\n",
       "         [7.68942475e-01, 1.47062910e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 1.03019905e+00, 1.85458049e-01],\n",
       "         [6.60330653e-01, 8.58195662e-01, 0.00000000e+00, ...,\n",
       "          5.26714146e-01, 9.46454048e-01, 0.00000000e+00],\n",
       "         [1.78587094e-01, 8.49398971e-01, 6.91891834e-02, ...,\n",
       "          8.68261158e-01, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[1.42874813e+00, 0.00000000e+00, 5.66674769e-01, ...,\n",
       "          3.36644113e-01, 2.25867510e+00, 0.00000000e+00],\n",
       "         [5.28068662e-01, 2.41930306e-01, 0.00000000e+00, ...,\n",
       "          7.88549066e-01, 3.45656347e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 1.11576211e+00, 0.00000000e+00, ...,\n",
       "          1.08674037e+00, 2.33060884e+00, 7.20527470e-01],\n",
       "         [0.00000000e+00, 8.46525192e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 2.38700271e-01, 8.37134182e-01],\n",
       "         [0.00000000e+00, 6.48640394e-02, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 9.23870444e-01, 4.07819837e-01, ...,\n",
       "          1.36592615e+00, 0.00000000e+00, 5.43898284e-01]],\n",
       "\n",
       "        [[1.12075758e+00, 0.00000000e+00, 2.57105172e-01, ...,\n",
       "          9.42003310e-01, 9.91547167e-01, 1.26096472e-01],\n",
       "         [6.32403135e-01, 0.00000000e+00, 3.94560039e-01, ...,\n",
       "          1.62719321e+00, 1.51517367e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          1.91845524e+00, 9.20094311e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          6.76704347e-01, 3.09583932e-01, 0.00000000e+00],\n",
       "         [3.68879378e-01, 0.00000000e+00, 1.88309938e-01, ...,\n",
       "          1.85832703e+00, 2.28459612e-01, 0.00000000e+00],\n",
       "         [4.04950887e-01, 3.59160870e-01, 9.94559675e-02, ...,\n",
       "          1.02736771e+00, 0.00000000e+00, 2.99938768e-01]]],\n",
       "\n",
       "\n",
       "       [[[7.57726073e-01, 4.78269279e-01, 2.89182156e-01, ...,\n",
       "          0.00000000e+00, 5.77444971e-01, 0.00000000e+00],\n",
       "         [3.77552390e-01, 8.46817791e-01, 2.21524775e-01, ...,\n",
       "          4.97461766e-01, 5.34188867e-01, 8.56971443e-02],\n",
       "         [0.00000000e+00, 1.26131773e+00, 0.00000000e+00, ...,\n",
       "          2.83341169e-01, 4.16179597e-01, 1.06382358e+00],\n",
       "         [1.00784743e+00, 1.13370395e+00, 0.00000000e+00, ...,\n",
       "          2.17416748e-01, 1.47419524e+00, 0.00000000e+00],\n",
       "         [3.92637312e-01, 1.50990188e+00, 0.00000000e+00, ...,\n",
       "          3.70646417e-02, 8.23083282e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 1.04876149e+00, 5.16032338e-01, ...,\n",
       "          3.89874637e-01, 0.00000000e+00, 1.63580582e-01]],\n",
       "\n",
       "        [[1.35706222e+00, 2.50392646e-01, 1.28529206e-01, ...,\n",
       "          9.73118424e-01, 1.06202078e+00, 2.96067968e-02],\n",
       "         [1.07852590e+00, 8.77265871e-01, 6.59916818e-01, ...,\n",
       "          1.70621288e+00, 1.20290744e+00, 1.39084756e-01],\n",
       "         [5.90643585e-01, 9.34118390e-01, 0.00000000e+00, ...,\n",
       "          1.90188920e+00, 1.07160127e+00, 1.56916189e+00],\n",
       "         [4.97220159e-01, 1.17705953e+00, 0.00000000e+00, ...,\n",
       "          6.03450716e-01, 9.71510947e-01, 1.98160660e+00],\n",
       "         [2.33068734e-01, 1.40483224e+00, 0.00000000e+00, ...,\n",
       "          5.74225426e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 1.26884186e+00, 0.00000000e+00, ...,\n",
       "          7.76317716e-01, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[1.14034641e+00, 1.46218777e-01, 5.61741710e-01, ...,\n",
       "          6.49898112e-01, 1.47306681e+00, 0.00000000e+00],\n",
       "         [9.33997154e-01, 1.09830832e+00, 1.10959172e+00, ...,\n",
       "          0.00000000e+00, 7.34269261e-01, 0.00000000e+00],\n",
       "         [1.25174806e-01, 9.12549317e-01, 0.00000000e+00, ...,\n",
       "          3.28876674e-02, 0.00000000e+00, 1.44126713e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          6.07391715e-01, 0.00000000e+00, 1.48584914e+00],\n",
       "         [0.00000000e+00, 6.99180126e-01, 0.00000000e+00, ...,\n",
       "          4.26554710e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 3.17238986e-01, 6.01000041e-02, ...,\n",
       "          4.56224084e-01, 0.00000000e+00, 2.51361698e-01]],\n",
       "\n",
       "        [[5.80825135e-02, 7.60677636e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 4.37212437e-01, 0.00000000e+00],\n",
       "         [6.84985280e-01, 8.07671130e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 1.14525783e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 1.33564508e+00, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 7.64890969e-01, 6.04996324e-01],\n",
       "         [0.00000000e+00, 1.19725275e+00, 0.00000000e+00, ...,\n",
       "          2.99072742e-01, 2.19196707e-01, 4.63523775e-01],\n",
       "         [0.00000000e+00, 6.35608971e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 3.00149262e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 4.64133471e-01, 0.00000000e+00, ...,\n",
       "          7.66643405e-01, 0.00000000e+00, 7.93660730e-02]],\n",
       "\n",
       "        [[2.26246268e-01, 1.88237891e-01, 2.33566716e-01, ...,\n",
       "          0.00000000e+00, 1.25859392e+00, 0.00000000e+00],\n",
       "         [1.34010124e+00, 9.20340538e-01, 7.31295884e-01, ...,\n",
       "          1.96400952e+00, 1.70905030e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 1.25383019e+00, 0.00000000e+00, ...,\n",
       "          1.84583437e+00, 1.14905369e+00, 9.88898337e-01],\n",
       "         [0.00000000e+00, 1.61289775e+00, 0.00000000e+00, ...,\n",
       "          9.40056622e-01, 5.16195774e-01, 1.09151697e+00],\n",
       "         [0.00000000e+00, 2.25601435e-01, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 1.70864153e+00],\n",
       "         [0.00000000e+00, 8.68908763e-01, 0.00000000e+00, ...,\n",
       "          8.78184676e-01, 0.00000000e+00, 6.23790801e-01]],\n",
       "\n",
       "        [[1.79624856e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          8.18204939e-01, 2.99267292e-01, 4.24002588e-01],\n",
       "         [1.10217623e-01, 0.00000000e+00, 7.92395353e-01, ...,\n",
       "          2.14755702e+00, 7.17249572e-01, 1.70226265e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "          2.84304762e+00, 9.26787376e-01, 5.17188847e-01],\n",
       "         [0.00000000e+00, 3.53877187e-01, 0.00000000e+00, ...,\n",
       "          2.15315700e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 3.93930912e-01, 5.94902158e-01, ...,\n",
       "          1.32630420e+00, 5.10512948e-01, 0.00000000e+00],\n",
       "         [2.11916253e-01, 2.19456255e-01, 4.88787889e-01, ...,\n",
       "          1.36721027e+00, 0.00000000e+00, 0.00000000e+00]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_batch_norm_48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning setup: classification with subsamples per object class (few shot learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer learning params\n",
    "# SAMPLES_PER_CLASS = [1, 5, 10, 20, 30, 40, 50] # NOTE: taking more samples per class since it is 88 for 50\n",
    "# SAMPLES_PER_CLASS = [10, 20, 30, 40, 50, 70, 90, 110, 130, 150, len(x_test)]\n",
    "SAMPLES_PER_CLASS = [10, 20, 30, 40, 50, 80, 110, 140, 170, 200]\n",
    "TRIALS = 10\n",
    "\n",
    "NUM_CLASSES_WATERTANK = 11\n",
    "NUM_CLASSES_TURNEDTABLE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten train & test data for SVM\n",
    "x_train_batch_norm_48 = flatten(x_train_batch_norm_48.numpy())\n",
    "x_train_batch_norm_49 = flatten(x_train_batch_norm_49.numpy())\n",
    "x_train_batch_norm_50 = flatten(x_train_batch_norm_50.numpy())\n",
    "\n",
    "x_test_batch_norm_48 = flatten(x_test_batch_norm_48.numpy())\n",
    "x_test_batch_norm_49 = flatten(x_test_batch_norm_49.numpy())\n",
    "x_test_batch_norm_50 = flatten(x_test_batch_norm_50.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1505,)\n",
      "(645,)\n"
     ]
    }
   ],
   "source": [
    "# these two are not modified (only x_test, x_train)\n",
    "print(y_train.numpy().shape)\n",
    "print(y_test.numpy().shape)\n",
    "\n",
    "y_train = y_train.numpy() # convert from tf tensor --> numpy\n",
    "y_test = y_test.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run svm tl evaluation with spc for each n-th layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_with_spc(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Takes embeddings from pretrained model and evaluates transfer learning \n",
    "    with few samples per class.\n",
    "    \"\"\"\n",
    "    # NOTE: svm takes original labels (not one-hot encoding)\n",
    "    for spc in SAMPLES_PER_CLASS:\n",
    "        accuracies = []\n",
    "\n",
    "        for i in range(TRIALS):\n",
    "            x_sample, y_sample = classSampling(x_train, y_train, spc, NUM_CLASSES_TURNEDTABLE)\n",
    "\n",
    "            svm = SVC(C=1.0, decision_function_shape = 'ovo', kernel=\"linear\")\n",
    "            svm.fit(x_sample, y_sample)\n",
    "\n",
    "            train_acc = svm.score(x_sample, y_sample)\n",
    "            test_acc = svm.score(x_test, y_test)\n",
    "\n",
    "            print(\"SPC {} Train Accuracy: {:.3f}\".format(spc, train_acc))\n",
    "            print(\"SPC {} Test Accuracy: {:.3f}\".format(spc, test_acc))\n",
    "            print()\n",
    "\n",
    "            accuracies.append(test_acc)\n",
    "\n",
    "        mean_acc = np.mean(accuracies)\n",
    "        std_acc = np.std(accuracies)\n",
    "\n",
    "        mean_acc = round(100 * mean_acc, 3)\n",
    "        std_acc = round(100 * std_acc, 3)\n",
    "\n",
    "        print(\"After {} trials - Test Accuracy is {} +- {}\".format(TRIALS, mean_acc, std_acc ))\n",
    "        print(\"------------------------------------------------------------------------------\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.594\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.586\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.580\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.552\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.605\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.600\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.572\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.597\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.577\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.560\n",
      "\n",
      "After 10 trials - Test Accuracy is 58.217 +- 1.657\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.679\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.653\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.678\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.653\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.651\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.690\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.650\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.674\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.690\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.698\n",
      "\n",
      "After 10 trials - Test Accuracy is 67.147 +- 1.751\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.667\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.727\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.710\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.721\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.736\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.674\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.721\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.729\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.726\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.721\n",
      "\n",
      "After 10 trials - Test Accuracy is 71.318 +- 2.233\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.736\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.749\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.726\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.749\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.761\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.730\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.757\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.757\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.752\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.750\n",
      "\n",
      "After 10 trials - Test Accuracy is 74.667 +- 1.131\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.752\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.743\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.752\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.791\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.798\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.757\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.772\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.774\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.778\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.803\n",
      "\n",
      "After 10 trials - Test Accuracy is 77.194 +- 1.991\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.819\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.814\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_svm_with_spc(x_train_batch_norm_48, y_train, x_test_batch_norm_48, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.620\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.681\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.713\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.691\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.688\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.690\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.662\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.699\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.712\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.729\n",
      "\n",
      "After 10 trials - Test Accuracy is 68.853 +- 2.884\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.812\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.795\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.795\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.805\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.775\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.783\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.778\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.795\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.783\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.775\n",
      "\n",
      "After 10 trials - Test Accuracy is 78.977 +- 1.215\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.854\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.843\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.839\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.814\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.867\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.823\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.837\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.828\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.833\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.845\n",
      "\n",
      "After 10 trials - Test Accuracy is 83.829 +- 1.446\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.874\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.848\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.865\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.870\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.881\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.876\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.850\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.879\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.881\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.867\n",
      "\n",
      "After 10 trials - Test Accuracy is 86.899 +- 1.133\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.870\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.881\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.879\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.882\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.912\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.882\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.859\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.904\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.874\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.876\n",
      "\n",
      "After 10 trials - Test Accuracy is 88.186 +- 1.464\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.907\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.926\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.904\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.922\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.922\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.916\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.909\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.891\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.893\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.913\n",
      "\n",
      "After 10 trials - Test Accuracy is 91.039 +- 1.133\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.927\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.929\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.933\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.932\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.927\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.921\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.909\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.929\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.933\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.919\n",
      "\n",
      "After 10 trials - Test Accuracy is 92.589 +- 0.73\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.938\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.946\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.941\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.938\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.950\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.930\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.944\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.926\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.943\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.935\n",
      "\n",
      "After 10 trials - Test Accuracy is 93.907 +- 0.704\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.950\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.938\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.955\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.944\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.941\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.949\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.938\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.952\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.950\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.961\n",
      "\n",
      "After 10 trials - Test Accuracy is 94.791 +- 0.718\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.950\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.941\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.950\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.950\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.957\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.949\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.947\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.943\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.936\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.943\n",
      "\n",
      "After 10 trials - Test Accuracy is 94.667 +- 0.56\n",
      "------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_svm_with_spc(x_train_batch_norm_49, y_train, x_test_batch_norm_49, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.648\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.684\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.628\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.670\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.647\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.625\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.662\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.679\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.640\n",
      "\n",
      "SPC 10 Train Accuracy: 1.000\n",
      "SPC 10 Test Accuracy: 0.569\n",
      "\n",
      "After 10 trials - Test Accuracy is 64.512 +- 3.174\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.722\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.738\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.744\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.707\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.718\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.741\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.729\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.732\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.724\n",
      "\n",
      "SPC 20 Train Accuracy: 1.000\n",
      "SPC 20 Test Accuracy: 0.712\n",
      "\n",
      "After 10 trials - Test Accuracy is 72.667 +- 1.179\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.811\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.780\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.766\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.795\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.769\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.794\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.771\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.784\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.780\n",
      "\n",
      "SPC 30 Train Accuracy: 1.000\n",
      "SPC 30 Test Accuracy: 0.778\n",
      "\n",
      "After 10 trials - Test Accuracy is 78.279 +- 1.316\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.803\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.797\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.809\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.812\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.809\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.802\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.806\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.840\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.806\n",
      "\n",
      "SPC 40 Train Accuracy: 1.000\n",
      "SPC 40 Test Accuracy: 0.811\n",
      "\n",
      "After 10 trials - Test Accuracy is 80.961 +- 1.115\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.809\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.833\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.847\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.833\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.845\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.840\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.843\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.853\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.833\n",
      "\n",
      "SPC 50 Train Accuracy: 1.000\n",
      "SPC 50 Test Accuracy: 0.839\n",
      "\n",
      "After 10 trials - Test Accuracy is 83.736 +- 1.132\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.871\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.865\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.881\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.842\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.864\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.864\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.854\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.854\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.859\n",
      "\n",
      "SPC 80 Train Accuracy: 1.000\n",
      "SPC 80 Test Accuracy: 0.878\n",
      "\n",
      "After 10 trials - Test Accuracy is 86.31 +- 1.101\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.890\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.867\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.879\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.893\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.876\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.887\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.890\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.870\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.874\n",
      "\n",
      "SPC 110 Train Accuracy: 1.000\n",
      "SPC 110 Test Accuracy: 0.901\n",
      "\n",
      "After 10 trials - Test Accuracy is 88.264 +- 1.052\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.876\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.884\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.890\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.891\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.887\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.898\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.887\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.890\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.887\n",
      "\n",
      "SPC 140 Train Accuracy: 1.000\n",
      "SPC 140 Test Accuracy: 0.898\n",
      "\n",
      "After 10 trials - Test Accuracy is 88.868 +- 0.608\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.907\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.910\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.913\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.905\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.893\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.895\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.905\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.907\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.899\n",
      "\n",
      "SPC 170 Train Accuracy: 1.000\n",
      "SPC 170 Test Accuracy: 0.901\n",
      "\n",
      "After 10 trials - Test Accuracy is 90.357 +- 0.619\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.899\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.893\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.905\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.918\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.895\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.902\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.904\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.905\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.901\n",
      "\n",
      "SPC 200 Train Accuracy: 1.000\n",
      "SPC 200 Test Accuracy: 0.895\n",
      "\n",
      "After 10 trials - Test Accuracy is 90.171 +- 0.691\n",
      "------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_svm_with_spc(x_train_batch_norm_50, y_train, x_test_batch_norm_50, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rotnet] *",
   "language": "python",
   "name": "conda-env-rotnet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
